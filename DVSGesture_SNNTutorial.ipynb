{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0PD5VPOUr4bs"
      },
      "source": [
        "[<img src='https://github.com/jeshraghian/snntorch/blob/master/docs/_static/img/snntorch_alpha_w.png?raw=true' width=\"300\">](https://github.com/jeshraghian/snntorch/)\n",
        "[<img src='https://github.com/neuromorphs/tonic/blob/develop/docs/_static/tonic-logo-white.png?raw=true' width=\"200\">](https://github.com/neuromorphs/tonic/)\n",
        "\n",
        "\n",
        "# Training the DVSGesture Dataset from Tonic + snnTorch Tutorial\n",
        "##### By Malachi Nguyen (mayanguy@ucsc.edu)\n",
        "##### A special thank you to Professor Jason Eshraghian and my Tutor Giridhar Vadhul for teaching patiently and inpsiring their students.\n",
        "\n",
        "<a href=\"https://colab.research.google.com/drive/1P2yQCDmp7TilNrEqj_cBzS7vscIs0L_o?usp=sharing\">\n",
        "  <img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/>\n",
        "</a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iawcPZ7DtDqK"
      },
      "source": [
        "For a comprehensive overview on how SNNs work, and what is going on under the hood, [then you might be interested in the snnTorch tutorial series available here.](https://snntorch.readthedocs.io/en/latest/tutorials/index.html)\n",
        "The snnTorch tutorial series is based on the following paper. If you find these resources or code useful in your work, please consider citing the following source:\n",
        "\n",
        "> <cite> [Jason K. Eshraghian, Max Ward, Emre Neftci, Xinxin Wang, Gregor Lenz, Girish Dwivedi, Mohammed Bennamoun, Doo Seok Jeong, and Wei D. Lu. \"Training Spiking Neural Networks Using Lessons From Deep Learning\". Proceedings of the IEEE, 111(9) September 2023.](https://ieeexplore.ieee.org/abstract/document/10242251) </cite>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Importing Dependencies and Libraries\n",
        "\n",
        "Please checkout the [Tonic](https://tonic.readthedocs.io/en/latest/) and [SNNTorch](https://snntorch.readthedocs.io/en/latest/tutorials/index.html) libraries for more information."
      ],
      "metadata": {
        "id": "R5GMBXZLN76v"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "W-v36rDBv41L",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a2a44d70-50a9-49c9-cef7-fdb3c114829d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m110.7/110.7 kB\u001b[0m \u001b[31m3.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m107.5/107.5 kB\u001b[0m \u001b[31m12.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m50.4/50.4 kB\u001b[0m \u001b[31m5.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m125.2/125.2 kB\u001b[0m \u001b[31m4.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m76.2/76.2 kB\u001b[0m \u001b[31m10.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m23.7/23.7 MB\u001b[0m \u001b[31m58.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m823.6/823.6 kB\u001b[0m \u001b[31m57.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m14.1/14.1 MB\u001b[0m \u001b[31m87.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m731.7/731.7 MB\u001b[0m \u001b[31m747.2 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m410.6/410.6 MB\u001b[0m \u001b[31m2.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m121.6/121.6 MB\u001b[0m \u001b[31m8.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.5/56.5 MB\u001b[0m \u001b[31m15.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m124.2/124.2 MB\u001b[0m \u001b[31m8.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m196.0/196.0 MB\u001b[0m \u001b[31m6.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m166.0/166.0 MB\u001b[0m \u001b[31m7.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m99.1/99.1 kB\u001b[0m \u001b[31m13.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.1/21.1 MB\u001b[0m \u001b[31m81.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ],
      "source": [
        "!pip install tonic --quiet\n",
        "!pip install snntorch --quiet"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "6WWIF2I1v7sA"
      },
      "outputs": [],
      "source": [
        "# tonic imports\n",
        "import tonic\n",
        "import tonic.transforms as transforms  # Not to be mistaken with torchdata.transfroms\n",
        "from tonic import DiskCachedDataset # alt: MemoryCachedDataset\n",
        "\n",
        "# torch imports\n",
        "import torch\n",
        "from torch.utils.data import random_split\n",
        "from torch.utils.data import DataLoader\n",
        "import torchvision\n",
        "import torch.nn as nn\n",
        "\n",
        "# snntorch imports\n",
        "import snntorch as snn\n",
        "from snntorch import surrogate\n",
        "import snntorch.spikeplot as splt\n",
        "from snntorch import functional as SF\n",
        "from snntorch import utils\n",
        "\n",
        "# other imports\n",
        "import matplotlib.pyplot as plt\n",
        "from IPython.display import HTML\n",
        "from IPython.display import display\n",
        "import numpy as np\n",
        "import torchdata\n",
        "import os\n",
        "from ipywidgets import IntProgress\n",
        "import time\n",
        "import statistics\n",
        "import itertools\n",
        "\n",
        "device = torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"mps\") if torch.backends.mps.is_available() else torch.device(\"cpu\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "McXriEu-tJV6"
      },
      "source": [
        "# 1. The Dataset - Loading and interpreting the DVSGesture Dataset\n",
        "\n",
        "The dataset used in this tutorial is DVSGesture from a team of researchers at IBM\n",
        "\n",
        "It is comprised of 11 classes, each being a gesture from a persons hands.\n",
        "(e.g: 1: hand clapping\n",
        "2: right hand wave\n",
        "3: left hand wave)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wsV-uUeZ6a2A"
      },
      "source": [
        "## 1.1 Loading the Dataset for SNN Torch\n",
        "\n",
        "1. The dataset presents data in a raw event format, so it must be shaped into a suitable format that can be fed into a model.\n",
        "\n",
        "\n",
        "2. The following code bins the raw DVS event data into 2000ms time windows, allowing the resulting tensors to be preprocessed before being put into the DataLoader. It also resizes the images from 128x128 pixels to 32x32 for effeciency and memory.\n",
        "\n",
        "\n",
        "3. One of the challenges in this segment is to figure out how you want to transform and load your data. Creating a dataloader function where you can specify the batch size and other parameters like transformattions allows for flexibility in handling different datasets and training configurations.\n",
        "\n",
        "*the code below was provided by the man, myth, and legend himself, Professor Jason Eshraghian\"\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "id": "dVcsackGOqiq"
      },
      "outputs": [],
      "source": [
        "def dataloader(config):\n",
        "    batch_size = config['batch_size']\n",
        "    # sensor_size = tonic.datasets.DVSGesture.sensor_size -- the default for DVSGesture Dataset is (128, 128, 2)\n",
        "    sensor_size = (32, 32, 2)\n",
        "\n",
        "    train_transform = transforms.Compose([transforms.Denoise(filter_time=10000),\n",
        "                                          transforms.Downsample(spatial_factor=0.25),\n",
        "                                          transforms.ToFrame(sensor_size=sensor_size,\n",
        "                                                             n_time_bins=config['train_time_bin']),\n",
        "                                          ])\n",
        "\n",
        "    test_transform = transforms.Compose([transforms.Denoise(filter_time=10000),\n",
        "                                        transforms.Downsample(spatial_factor=0.25),\n",
        "                                        transforms.ToFrame(sensor_size=sensor_size,\n",
        "                                                            n_time_bins=config['test_time_bin']),\n",
        "                                        ])\n",
        "\n",
        "    trainset = tonic.datasets.DVSGesture(save_to=config['data_dir'], transform=train_transform, train=True)\n",
        "    testset = tonic.datasets.DVSGesture(save_to=config['data_dir'], transform=test_transform, train=False)\n",
        "\n",
        "    cached_trainset = DiskCachedDataset(trainset, cache_path='./data/cache/dvs/train')\n",
        "    cached_testset = DiskCachedDataset(testset, cache_path='./data/cache/dvs/test')\n",
        "\n",
        "    train_loader = DataLoader(cached_trainset, batch_size=batch_size, collate_fn=tonic.collation.PadTensors(batch_first=False))\n",
        "    test_loader = DataLoader(cached_testset, batch_size=batch_size, collate_fn=tonic.collation.PadTensors(batch_first=False))\n",
        "\n",
        "    return train_loader, test_loader"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now we call our dataloader function to correctly load our data with the correct parameters. You can and should edit this to test different levels of effeciency and accuracy while testing."
      ],
      "metadata": {
        "id": "Y7D4CmwQQepm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Define configuration parameters\n",
        "config = {\n",
        "    'batch_size': 7,\n",
        "    'data_dir': './data',  # Specify your data directory\n",
        "    'train_time_bin': 2000,  # Time bin for training data\n",
        "    'test_time_bin': 2000,  # Time bin for testing data\n",
        "}\n",
        "\n",
        "# Call the dataloader function\n",
        "train_loader, test_loader = dataloader(config)\n",
        "\n",
        "# Check the length of the loaders for more information\n",
        "print(f\"Number of batches in train loader: {len(train_loader)}\")\n",
        "print(f\"Number of batches in test loader: {len(test_loader)}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JjIzXNykJhcU",
        "outputId": "fbdb9c59-e06f-4585-8de6-2a136eeedda8"
      },
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Number of batches in train loader: 154\n",
            "Number of batches in test loader: 38\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ickp0FA4_nBR"
      },
      "source": [
        "## 1.2 Visualizing your data\n",
        "\n",
        "It's very important to visualize your data so you know what you're working with. How will you be able to conceptually grasp what to learn if you don't know what it looks like?\n",
        "\n",
        "Here we can visualize the frames."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "r7VXRd6RPfP_"
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Define the number of rows and columns for subplots\n",
        "num_rows = 2\n",
        "num_cols = 10\n",
        "\n",
        "fig, ax = plt.subplots(num_rows, num_cols, figsize=(20, 4))\n",
        "\n",
        "for i, (data, targets) in enumerate(train_loader):\n",
        "    if i >= num_rows * num_cols:\n",
        "        break\n",
        "\n",
        "    row = i // num_cols\n",
        "    col = i % num_cols\n",
        "    ax[row, col].imshow(data[0][0][0])\n",
        "    ax[row, col].axis('off')\n",
        "\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Another very important thing to visualize is the size and shape of your data. Below is how you can see information on your image (Batchsize, channels, img height, img_width). This information is very important for understanding what sizes to use for your convolutional layering. Loading and transforming in the previous cell allows us understand and see the attributes of our new transformed set."
      ],
      "metadata": {
        "id": "lC8Tol8jJk5A"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#printing image size and checking an image within the batch\n",
        "print(\"Input image size:\", data.shape)\n",
        "\n",
        "image_index = 1 #make any number you want to check the images index in your batch\n",
        "image_size = data[image_index].shape\n",
        "print(\"Size of the first image in the batch:\", image_size)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9YE3zMllJjfM",
        "outputId": "aea322ad-3ed7-42dd-9885-b16347ef7072"
      },
      "execution_count": 38,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Input image size: torch.Size([2000, 7, 2, 32, 32])\n",
            "Size of the first image in the batch: torch.Size([7, 2, 32, 32])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BwEPwyzqSs14"
      },
      "source": [
        "#2.1 Defining the Network\n",
        "\n",
        "The model used is a sequential network comprised of two sets of convolution layers with 5x5 filters, followed by a final linear and leaky output layer that convert the 800 tensor into 11 output classes.\n",
        "\n",
        "The forward function gets the spikes from one batch of data and returns them as a tensor.\n",
        "\n",
        "Please see [this link](https://snntorch.readthedocs.io/en/latest/tutorials/tutorial_6.html#define-the-network) for an in depth explaination of this Network\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 39,
      "metadata": {
        "id": "9ejkKRxsSrb4"
      },
      "outputs": [],
      "source": [
        "#parameters\n",
        "num_classes = 11\n",
        "spike_grad = surrogate.atan() # arctan surrogate gradient function\n",
        "beta = 0.5\n",
        "\n",
        "net = nn.Sequential(nn.Conv2d(2, 12, 5), # first conv layer\n",
        "                        snn.Leaky(beta=beta, spike_grad=spike_grad, init_hidden=True),\n",
        "                        nn.MaxPool2d(2),\n",
        "                        nn.Conv2d(12, 32, 5), # second conv layer\n",
        "                        snn.Leaky(beta=beta, spike_grad=spike_grad, init_hidden=True),\n",
        "                        nn.MaxPool2d(2),\n",
        "                        nn.Flatten(),\n",
        "                        nn.Linear(32*5*5, num_classes), #flattened linear layer\n",
        "                        snn.Leaky(beta=beta, spike_grad=spike_grad, init_hidden=True, output=True)\n",
        "                        ).to(device)\n",
        "\n",
        "\n",
        "#Record the membrane potential and spike response over time:\n",
        "\n",
        "def forward(net, data):\n",
        "  spk_rec = []\n",
        "  utils.reset(net)  # resets hidden states for all LIF neurons in net\n",
        "  for step in range(data.size(0)):\n",
        "      spk_out, mem_out = net(data[step])\n",
        "      spk_rec.append(spk_out)\n",
        "  return torch.stack(spk_rec)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 40,
      "metadata": {
        "id": "u_iPLE27TEWm"
      },
      "outputs": [],
      "source": [
        "optimizer = torch.optim.Adam(net.parameters(), lr=0.003, betas=(0.9, 0.999)) # learning rate = 0.003\n",
        "loss_fn = SF.mse_count_loss(correct_rate=0.8, incorrect_rate=0.2) # MSE loss function"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "baA0kllPW6vf"
      },
      "source": [
        "#2.2 Training\n",
        "\n",
        "We have loaded and converted our dataset into readable and iterable data for SNNs. We will now train it by iterating over our training batches and using that to create predictions for future data."
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Training Notes and Challenges:\n",
        "1.   This takes a LONG time... be patient and tweak the number of iterations, epochs and batch sizes to find the optimal balance between speed and accuracy that works for you.  \n",
        "2.   You may have issues with layer sizes and dimension compatibilty. Issues with multiplying mat1 and mat2 most likely stem from how you defined the layers in your network. Make sure your input tensor size matches the expected input size of the linear layer.\n",
        "3.   It takes awhile to train.\n",
        "4.   You may run out of RAM or your computer cannot run this training due to runtiume errors or things of that nature. This could be from how you load your data. Make it size smaller, chop it up, or resize it however needed specific to your dataset. Printing the sizes and loading frames of your data is an easy way to see how you may need to load your data.\n",
        "5.  Still waiting for training to finish 41 mins in\n",
        "6.  Checkout this document for a detailed log of my trails and tribulations to optimize the epochs and iterations. This may help you figure out a good number to train your dataset!\n",
        "7. Training\n",
        "\n"
      ],
      "metadata": {
        "id": "WukFXYVOSNbl"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AjXT2xJrXJ9s",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "33baeba8-7799-418d-8a04-c9a42fb22f29"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 0, Iteration 0 \n",
            "Train Loss: 189.09\n",
            "Accuracy: 14.29%\n",
            "\n",
            "========== Test Set Accuracy: 9.47% ==========\n",
            "\n",
            "Epoch 0, Iteration 1 \n",
            "Train Loss: 189.09\n",
            "Accuracy: 14.29%\n",
            "\n",
            "========== Test Set Accuracy: 12.12% ==========\n",
            "\n",
            "Epoch 0, Iteration 2 \n",
            "Train Loss: 177.33\n",
            "Accuracy: 0.00%\n",
            "\n",
            "========== Test Set Accuracy: 13.26% ==========\n",
            "\n",
            "Epoch 0, Iteration 3 \n",
            "Train Loss: 101.47\n",
            "Accuracy: 28.57%\n",
            "\n",
            "========== Test Set Accuracy: 13.26% ==========\n",
            "\n",
            "Epoch 0, Iteration 4 \n",
            "Train Loss: 126.47\n",
            "Accuracy: 0.00%\n",
            "\n",
            "========== Test Set Accuracy: 28.03% ==========\n",
            "\n",
            "Epoch 0, Iteration 5 \n",
            "Train Loss: 97.30\n",
            "Accuracy: 28.57%\n",
            "\n",
            "========== Test Set Accuracy: 32.95% ==========\n",
            "\n",
            "Epoch 0, Iteration 6 \n",
            "Train Loss: 101.99\n",
            "Accuracy: 42.86%\n",
            "\n",
            "========== Test Set Accuracy: 33.71% ==========\n",
            "\n",
            "Epoch 0, Iteration 7 \n",
            "Train Loss: 106.31\n",
            "Accuracy: 14.29%\n",
            "\n",
            "========== Test Set Accuracy: 30.30% ==========\n",
            "\n",
            "Epoch 0, Iteration 8 \n",
            "Train Loss: 118.05\n",
            "Accuracy: 71.43%\n",
            "\n",
            "========== Test Set Accuracy: 30.30% ==========\n",
            "\n",
            "Epoch 0, Iteration 9 \n",
            "Train Loss: 138.93\n",
            "Accuracy: 42.86%\n",
            "\n"
          ]
        }
      ],
      "source": [
        "num_epochs = 20 #number of complete passes over the entire dataset\n",
        "num_iters = 6\n",
        "\n",
        "loss_hist = []\n",
        "acc_hist = []\n",
        "test_acc_hist = []\n",
        "\n",
        "# training loop\n",
        "for epoch in range(num_epochs):\n",
        "    for i, (data, targets) in enumerate(iter(train_loader)):\n",
        "        data = data.to(device)\n",
        "        targets = targets.to(device)\n",
        "\n",
        "        net.train()\n",
        "        spk_rec = forward(net, data)\n",
        "        loss_val = loss_fn(spk_rec, targets)\n",
        "\n",
        "        # Gradient calculation + weight update\n",
        "        optimizer.zero_grad()\n",
        "        loss_val.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        # Store loss history for future plotting\n",
        "        loss_hist.append(loss_val.item())\n",
        "\n",
        "        print(f\"Epoch {epoch}, Iteration {i} \\nTrain Loss: {loss_val.item():.2f}\")\n",
        "\n",
        "        acc = SF.accuracy_rate(spk_rec, targets)\n",
        "        acc_hist.append(acc)\n",
        "        print(f\"Accuracy: {acc * 100:.2f}%\\n\")\n",
        "\n",
        "        correct = 0\n",
        "        total = 0\n",
        "        for i, (test_data, test_targets) in enumerate(iter(test_loader)):\n",
        "            test_data = test_data.to(device)\n",
        "            test_targets = test_targets.to(device)\n",
        "            spk_rec = forward(net, test_data)\n",
        "            correct += SF.accuracy_rate(spk_rec, test_targets) * spk_rec.size(1)\n",
        "            total += spk_rec.size(1)\n",
        "\n",
        "        test_acc = (correct/total) * 100\n",
        "        test_acc_hist.append(test_acc)\n",
        "        print(f\"========== Test Set Accuracy: {test_acc:.2f}% ==========\\n\")\n",
        "\n",
        "        if i == num_iters:\n",
        "          break"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "n4EztwU0aCOS"
      },
      "source": [
        "#3 Results and Fine Tuning\n",
        "\n",
        "#3.1 Visualizing your results\n",
        "\n",
        "Visualizing your results is an important step in creating a powerful and accurate SNN. Plotting it on a graph allows you to see which direction you are headed.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mzafiRUIaclq"
      },
      "outputs": [],
      "source": [
        "# Plot Loss\n",
        "fig = plt.figure(facecolor=\"w\")\n",
        "plt.plot(loss_hist)\n",
        "plt.title(\"Train Set Loss\")\n",
        "plt.xlabel(\"Iteration\")\n",
        "plt.ylabel(\"Loss\")\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-q4vGuzAagjM"
      },
      "outputs": [],
      "source": [
        "# Plot Train Accuracy\n",
        "fig = plt.figure(facecolor=\"w\")\n",
        "plt.plot(acc_hist)\n",
        "plt.title(\"Train Set Accuracy\")\n",
        "plt.xlabel(\"Iteration\")\n",
        "plt.ylabel(\"Accuracy\")\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gL_78y8hakOf"
      },
      "outputs": [],
      "source": [
        "# Plot Test Accuracy\n",
        "fig = plt.figure(facecolor=\"w\")\n",
        "plt.plot(test_acc_hist)\n",
        "plt.title(\"Test Set Accuracy\")\n",
        "plt.xlabel(\"Iteration\")\n",
        "plt.ylabel(\"Accuracy\")\n",
        "plt.show()"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}